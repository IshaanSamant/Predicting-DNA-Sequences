{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BioLSTM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"1VESUr3uC9U3"},"source":["import numpy as np\n","import gc\n","import os,logging,pickle,random\n","from matplotlib import pyplot\n","import pandas as pd\n","from scipy import stats\n","import keras\n","import h5py\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tensorflow.keras import layers\n","from tensorflow.keras.optimizers import SGD, Adam\n","from tensorflow.keras.models import Sequential, Model, load_model\n","from tensorflow.keras.layers import Dense, Activation, LSTM, Flatten, Dropout, Input, Concatenate\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2aAoWmhoD8Ih"},"source":["class BioLSTM:\n","    def __init__(self, model_type=\"tf\", n_epochs=50, batch_size=128, timestep=210, features=64, datadir='Dataset/embedded_data', opt='adam', lr=3e-4):\n","        \n","        self.batch_size = batch_size\n","        self.model_type = model_type\n","        self.n_epochs = n_epochs\n","        self.timestep = timestep\n","        self.features = features\n","        self.datadir = datadir\n","        self._build_model()\n","        self.opt = opt\n","        self.lr = lr\n","    def _build_model(self):\n","           \n","        if self.model_type == \"tf\":\n","          input_promoter = Input(shape=(self.timestep, self.features), name='promoter')\n","          halflife = Input(shape=(8,), name = 'halflife')\n","          tf = Input(shape=(181,), name = 'tf')\n","\n","          x = layers.BatchNormalization()(input_promoter)\n","          x = LSTM(units = 100, input_shape=(self.timestep,self.features))(x)\n","          #y = layers.BatchNormalization()(halflife)\n","          c = Concatenate()([x, halflife, tf])\n","          x = layers.Dense(90, activation=\"relu\")(c)\n","          x = layers.Dropout(0.3)(x)\n","          output = layers.Dense(1, activation=\"linear\")(x)\n","\n","          self.model = Model(inputs=(input_promoter,halflife,tf), outputs=output)\n","\n","        if self.model_type == \"classic\":\n","          input_promoter = Input(shape=(self.timestep, self.features), name='promoter')\n","          halflife = Input(shape=(8,), name = 'halflife')\n","\n","          x = layers.BatchNormalization()(input_promoter)\n","          x = LSTM(units = 100, input_shape=(self.timestep,self.features))(x)\n","          #y = layers.BatchNormalization()(halflife)\n","          c = Concatenate()([x, halflife])\n","          x = layers.Dense(90, activation=\"relu\")(c)\n","          x = layers.Dropout(0.3)(x)\n","          output = layers.Dense(1, activation=\"linear\")(x)\n","\n","          self.model = Model(inputs=(input_promoter,halflife), outputs=output)\n","\n","        if self.model_type == \"only promoter\":\n","          input_promoter = Input(shape=(self.timestep, self.features), name='promoter')\n","\n","          x = layers.BatchNormalization()(input_promoter)\n","          x = LSTM(units = 100, input_shape=(self.timestep,self.features))(x)\n","          x = layers.Dense(90, activation=\"relu\")(x)\n","          x = layers.Dropout(0.3)(x)\n","          output = layers.Dense(1, activation=\"linear\")(x)\n","\n","          self.model = Model(inputs=(input_promoter), outputs=output)\n","          \n","        print(self.model.summary())\n","        print(f\"\\nParameters:\\n{vars(self)}\\n\")\n","\n","    def train_model(self, x, y, x_v=None, y_v=None):\n","      if self.opt == 'adam':\n","        self.model.compile(optimizer=Adam(learning_rate=self.lr), loss='mse')\n","      if self.opt == 'sgd':\n","        self.model.compile(optimizer=SGD(learning_rate=self.lr, momentum=0.9), loss='mse')\n","      earlystop_cb = EarlyStopping(monitor='val_loss', patience=15, verbose=1, mode='min')\n","      check_cb = ModelCheckpoint(os.path.join(self.datadir, 'best.h5'), monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","      history = self.model.fit(x, y, batch_size=128, epochs=self.n_epochs, verbose=1, validation_data=(x_v,y_v),callbacks=[earlystop_cb,check_cb])\n","\n","      plt.rcParams[\"figure.figsize\"] = (20,9)\n","      pyplot.plot(history.history['loss'])\n","      pyplot.plot(history.history['val_loss'])\n","      pyplot.hlines(0.4, 0, len(history.history['loss']) , alpha = 0.2)\n","      pyplot.hlines(0.42, 0, len(history.history['loss']) , alpha = 0.2 )\n","      pyplot.title('model train vs validation loss')\n","      pyplot.ylabel('loss')\n","      pyplot.xlabel('epoch')\n","      pyplot.legend(['train', 'validation'], loc='upper right')\n","      pyplot.show()\n","        \n","    def evaluate(self, x, y):\n","        predictions = self.model.predict(x).flatten()\n","        slope, intercept, r_value, p_value, std_err = stats.linregress(predictions, y)\n","        print('Test R^2 = %.3f' % r_value**2)\n","        return r_value**2\n","\n","    def evaluate_best(self, x, y):\n","        best_file = os.path.join(self.datadir, 'best.h5')\n","        self.model = load_model(best_file)\n","        predictions = self.model.predict(x).flatten()\n","        slope, intercept, r_value, p_value, std_err = stats.linregress(predictions, y)\n","        print('Test R^2 = %.3f' % r_value**2)\n","        return r_value**2\n","\n","    def plot_kde(self, x, y, TPU=False):\n","        if TPU is False:\n","            best_file = os.path.join(self.datadir, 'best.h5')\n","            model = load_model(best_file)\n","            predictions = model.predict(x).flatten()\n","        else:\n","            predictions = self.model.predict(x).flatten()\n","        df = pd.DataFrame({\"predictions\":predictions, \"true\":y})\n","        ax = sns.displot(data=df, kde=True)\n","        plt.xlabel('Labels')\n","        plt.show()\n","        \n","    def plot_r2(self, x, y, TPU=False):\n","        from matplotlib import cm\n","        if TPU == False:\n","            best_file = os.path.join(self.datadir, 'best.h5')\n","            model = load_model(best_file)\n","            predictions = model.predict(x).flatten()\n","        else:\n","            predictions = self.model.predict(x).flatten()\n","        slope, intercept, r_value, p_value, std_err = stats.linregress(predictions, y)\n","\n","        viridis = cm.get_cmap('autumn', 12)\n","        diff = y - predictions\n","        diff = np.abs(diff)\n","\n","        ### plt size\n","        plt.rcParams[\"figure.figsize\"] = (10,9)\n","        ### plt fontsize\n","        plt.rcParams.update({'font.size': 16})\n","\n","        ### set title\n","        plt.title(\"Expression Scatterplot\")\n","        ### plot\n","        bis = np.arange(-1.5, 3, 2)\n","        plt.plot(bis, bis,  f\"b\", alpha=0.3)\n","        for p, yi, c in zip(predictions, y, diff):\n","            plt.plot(p, yi,  f\".\", markersize=10, color=viridis((1.0-c)/1.1))\n","        ### set ticks\n","        plt.xticks([i for i in range(-1, 4)])\n","        plt.yticks([i for i in range(-1, 4)])\n","        ### set labels\n","        plt.xlabel(\"Predicted expression level\")\n","        plt.ylabel(\"Median expression level\")\n","        ### create legend\n","        plt.legend(loc=\"upper right\", title=f\"r2 = %.3f\\n n = 1000\" % r_value**2)\n","        ### set ylim\n","        plt.ylim((-1.5,3))\n","        plt.xlim((-1.5,3))\n","        ### grid\n","        plt.grid(alpha=0.5)\n","        plt.show()\n"],"execution_count":null,"outputs":[]}]}